{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maziger/master-generative-ai-with-llm/blob/main/Notebooks/optimizing_RAG_GAI_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing RAG\n",
        "\n",
        "Authored by [Jesper N. Wulff](https://www.au.dk/jwulff@econ.au.dk/)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook teaches you how to build, evaluate, and optimize Retrieval-Augmented Generation (RAG) pipelines using the **Document Haystack** benchmark. You'll learn how different components affect RAG performance and discover what really matters when building production-quality retrieval systems.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- **Build a modular RAG pipeline** with swappable components (chunking, embedding, retrieval, generation)\n",
        "- **Evaluate RAG systems** using the \"needle in a haystack\" benchmark on real financial documents\n",
        "- **Understand performance bottlenecks** by separating retrieval quality from generation quality\n",
        "- **Experiment with improvements** like different embedding models, chunk sizes, and reranking\n",
        "- **Measure what matters** - accuracy, latency, and the trade-offs between them\n",
        "\n",
        "## The Needle in a Haystack Task\n",
        "\n",
        "We'll test RAG systems on a challenging benchmark: finding specific \"needles\" (hidden facts like \"The secret fruit is grape\") buried in long documents ranging from 5 to 200 pages. This simulates real-world scenarios where users need to find specific information in large document collections.\n",
        "\n",
        "**Key Question**: Can your RAG system find the needle, and can your LLM extract the right answer?\n",
        "\n",
        "## What Makes This Different?\n",
        "\n",
        "Unlike toy examples, this notebook:\n",
        "- Uses **real financial documents** (Goldman Sachs annual reports)\n",
        "- Tests performance on **documents up to 200 pages** long\n",
        "- Provides **two evaluation modes**: retrieval-only (tests if you can find the right chunks) vs. full RAG (tests if the LLM can extract the answer)\n",
        "- Makes it **easy to experiment** - change one component, see the impact immediately\n",
        "- Shows you **where the bottleneck is** - is it retrieval or generation?\n",
        "\n",
        "Let's dive in! ðŸš€\n"
      ],
      "metadata": {
        "id": "b21phGiKgly5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and installation\n",
        "\n",
        "First, install the nessecary libraries. If you get this error message:\n",
        "\n",
        "```\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.\n",
        "```\n",
        "\n",
        "Don't worry. Everything works just fine."
      ],
      "metadata": {
        "id": "0tQxUQMShaYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub pypdf langchain-community sentence-transformers transformers accelerate"
      ],
      "metadata": {
        "id": "mIKLGBo2qDjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Functions\n",
        "\n",
        "### `download_documents(document_name, cache_dir)`\n",
        "\n",
        "Downloads PDFs and metadata from the [Document Haystack](https://huggingface.co/datasets/AmazonScience/document-haystack) dataset.\n",
        "\n",
        "**Parameters:**\n",
        "- `document_name` (str): Name of the company/document to download. Options include:\n",
        "  - Specific companies: `\"GoldmanSachs\"`, `\"Tesla\"`, `\"JPMorgan\"`, `\"Disney\"`, etc.\n",
        "  - `\"all\"`: Downloads all 25 available company documents\n",
        "- `cache_dir` (str): Local directory where files will be stored (default: `\"./haystack_data\"`)\n",
        "\n",
        "**What it downloads:**\n",
        "- PDF files with embedded text needles (one per document length: 5, 10, 25, 50, 75, 100, 150, 200 pages)\n",
        "- `needles.csv`: The hidden facts to find\n",
        "- `prompt_questions.txt`: Questions to ask about each needle\n",
        "\n",
        "**Example usage:**\n",
        "```python\n",
        "# Download just Goldman Sachs documents\n",
        "base_path = download_documents(\"GoldmanSachs\")\n",
        "\n",
        "# Download Tesla documents\n",
        "base_path = download_documents(\"Tesla\")\n",
        "\n",
        "# Download everything (takes longer!)\n",
        "base_path = download_documents(\"all\")"
      ],
      "metadata": {
        "id": "rYpz3gXqhidm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from huggingface_hub import hf_hub_download\n",
        "from pathlib import Path\n",
        "import re\n",
        "import time\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# ================================================================================\n",
        "# 1. DATA LOADING\n",
        "# ================================================================================\n",
        "\n",
        "def download_documents(document_name=\"GoldmanSachs\", cache_dir=\"./haystack_data\"):\n",
        "    \"\"\"\n",
        "    Download PDFs and metadata from the Document Haystack dataset.\n",
        "\n",
        "    Args:\n",
        "        document_name: Name of document to download (e.g., \"GoldmanSachs\", \"AIG\", \"AmericanAirlines\")\n",
        "                      Or \"all\" to download all available documents\n",
        "        cache_dir: Local directory to store downloaded files\n",
        "\n",
        "    Returns:\n",
        "        Path to the base directory containing downloaded documents\n",
        "    \"\"\"\n",
        "    # Available documents in the dataset\n",
        "    all_documents = [\n",
        "        \"GoldmanSachs\", \"AIG\", \"AmericanAirlines\", \"APA\", \"BankOfMontreal\",\n",
        "        \"BristolMyers\", \"CVS\", \"Chevron\", \"Cigna\", \"Chubb\", \"Comcast\",\n",
        "        \"ConocoPhillips\", \"Disney\", \"ExxonMobil\", \"FedEx\", \"Ford\",\n",
        "        \"GeneralMotors\", \"HCA\", \"JPMorgan\", \"JohnsonJohnson\", \"Lowes\",\n",
        "        \"MetLife\", \"Progressive\", \"Tesla\", \"UnitedHealth\"\n",
        "    ]\n",
        "\n",
        "    if document_name == \"all\":\n",
        "        documents_to_download = all_documents\n",
        "    else:\n",
        "        if document_name not in all_documents:\n",
        "            print(f\"Warning: {document_name} not in known documents. Attempting anyway...\")\n",
        "        documents_to_download = [document_name]\n",
        "\n",
        "    page_lengths = [5, 10, 25, 50, 75, 100, 150, 200]\n",
        "    base_path = Path(cache_dir)\n",
        "    base_path.mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"Downloading documents: {', '.join(documents_to_download)}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for doc_name in documents_to_download:\n",
        "        print(f\"\\nðŸ“„ Downloading {doc_name}...\")\n",
        "\n",
        "        for pages in page_lengths:\n",
        "            folder_name = f\"{doc_name}_{pages}Pages\"\n",
        "\n",
        "            try:\n",
        "                # Download PDF with text needles\n",
        "                hf_hub_download(\n",
        "                    repo_id=\"AmazonScience/document-haystack\",\n",
        "                    repo_type=\"dataset\",\n",
        "                    filename=f\"{doc_name}/{folder_name}/{doc_name}_{pages}Pages_TextNeedles.pdf\",\n",
        "                    local_dir=str(base_path),\n",
        "                    local_dir_use_symlinks=False\n",
        "                )\n",
        "\n",
        "                # Download needles.csv\n",
        "                hf_hub_download(\n",
        "                    repo_id=\"AmazonScience/document-haystack\",\n",
        "                    repo_type=\"dataset\",\n",
        "                    filename=f\"{doc_name}/{folder_name}/needles.csv\",\n",
        "                    local_dir=str(base_path),\n",
        "                    local_dir_use_symlinks=False\n",
        "                )\n",
        "\n",
        "                # Download prompt_questions.txt\n",
        "                hf_hub_download(\n",
        "                    repo_id=\"AmazonScience/document-haystack\",\n",
        "                    repo_type=\"dataset\",\n",
        "                    filename=f\"{doc_name}/{folder_name}/prompt_questions.txt\",\n",
        "                    local_dir=str(base_path),\n",
        "                    local_dir_use_symlinks=False\n",
        "                )\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   âœ— Error downloading {pages}-page document: {e}\")\n",
        "\n",
        "        print(f\"   âœ“ {doc_name} downloaded\")\n",
        "\n",
        "    return base_path"
      ],
      "metadata": {
        "id": "NXMzvpOIfHAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `load_test_cases(base_path, document_name)`\n",
        "\n",
        "Loads test cases from downloaded documents and prepares them for evaluation.\n",
        "\n",
        "**Parameters:**\n",
        "- `base_path` (Path): Path to the directory containing downloaded documents (returned by `download_documents()`)\n",
        "- `document_name` (str): Name of document to load, or `\"all\"` to load from all downloaded documents\n",
        "\n",
        "**What it does:**\n",
        "- Reads PDFs and extracts full document text\n",
        "- Parses `needles.csv` to get the hidden facts (e.g., \"The secret fruit is grape\")\n",
        "- Loads corresponding questions from `prompt_questions.txt`\n",
        "- Extracts the key and expected value from each needle\n",
        "- Creates a test case for each needle across all document lengths\n",
        "\n",
        "**Returns:**\n",
        "A list of dictionaries, where each dictionary contains:\n",
        "- `document_name`: Company name (e.g., \"GoldmanSachs\")\n",
        "- `document_length`: Number of pages (5, 10, 25, 50, 75, 100, 150, 200)\n",
        "- `needle`: Full needle text (e.g., \"The secret fruit is a 'grape'.\")\n",
        "- `key`: What we're looking for (e.g., \"fruit\")\n",
        "- `expected_value`: The answer we expect (e.g., \"grape\")\n",
        "- `prompt`: The question to ask (e.g., \"What is the secret fruit in the document?\")\n",
        "- `full_document`: Complete text of the PDF\n",
        "\n",
        "**Example usage:**\n",
        "```python\n",
        "# Load Goldman Sachs test cases\n",
        "test_cases = load_test_cases(base_path, \"GoldmanSachs\")\n",
        "print(f\"Loaded {len(test_cases)} test cases\")  # Output: 165 test cases (5+10+25+25+...)\n",
        "\n",
        "# Load from all documents\n",
        "test_cases = load_test_cases(base_path, \"all\")"
      ],
      "metadata": {
        "id": "U8vNBp-rmDjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_cases(base_path, document_name=\"GoldmanSachs\"):\n",
        "    \"\"\"\n",
        "    Load test cases from downloaded documents.\n",
        "    \"\"\"\n",
        "    test_cases = []\n",
        "    page_lengths = [5, 10, 25, 50, 75, 100, 150, 200]\n",
        "\n",
        "    print(f\"Looking for documents in: {base_path}\")\n",
        "\n",
        "    # The files are in base_path/DocumentName/DocumentName_XPages/\n",
        "\n",
        "    doc_base = base_path / document_name\n",
        "\n",
        "    if not doc_base.exists():\n",
        "        print(f\"ERROR: Document folder not found at {doc_base}\")\n",
        "        print(f\"Available folders: {list(base_path.iterdir())}\")\n",
        "        return test_cases\n",
        "\n",
        "    print(f\"\\nProcessing {document_name}...\")\n",
        "\n",
        "    for pages in page_lengths:\n",
        "        folder_name = f\"{document_name}_{pages}Pages\"\n",
        "        folder_path = doc_base / folder_name\n",
        "\n",
        "        if not folder_path.exists():\n",
        "            continue\n",
        "\n",
        "        pdf_path = folder_path / f\"{document_name}_{pages}Pages_TextNeedles.pdf\"\n",
        "        needles_csv_path = folder_path / \"needles.csv\"\n",
        "        prompts_path = folder_path / \"prompt_questions.txt\"\n",
        "\n",
        "        if not pdf_path.exists() or not needles_csv_path.exists():\n",
        "            print(f\"  âœ— Missing files in {folder_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  âœ“ Loading {pages}-page document...\")\n",
        "\n",
        "        # Load PDF\n",
        "        loader = PyPDFLoader(str(pdf_path))\n",
        "        docs = loader.load()\n",
        "        full_document = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Read needles and prompts\n",
        "        needles_df = pd.read_csv(needles_csv_path, header=None, names=[\"needle_text\"])\n",
        "        with open(prompts_path, 'r') as f:\n",
        "            prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "        # Extract expected answers\n",
        "        for idx, needle in enumerate(needles_df[\"needle_text\"]):\n",
        "            match = re.search(r'The secret (.+?) is [\"\\']?(.+?)[\"\\']?\\.?$', needle)\n",
        "            if match and idx < len(prompts):\n",
        "                key = match.group(1)\n",
        "                value = match.group(2).strip('.\"\\'')\n",
        "\n",
        "                test_cases.append({\n",
        "                    \"document_name\": document_name,\n",
        "                    \"document_length\": pages,\n",
        "                    \"needle\": needle,\n",
        "                    \"key\": key,\n",
        "                    \"expected_value\": value,\n",
        "                    \"prompt\": prompts[idx],\n",
        "                    \"full_document\": full_document\n",
        "                })\n",
        "\n",
        "        print(f\"    Added {len(needles_df)} test cases\")\n",
        "\n",
        "    print(f\"\\nâœ“ Total test cases loaded: {len(test_cases)}\")\n",
        "    return test_cases"
      ],
      "metadata": {
        "id": "Phb0mAw0hzDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular RAG Components\n",
        "\n",
        "The RAG pipeline is built from swappable components. Each component has a single responsibility, making it easy to experiment with different configurations.\n",
        "\n",
        "### `Chunker`\n",
        "\n",
        "Splits long documents into smaller, manageable chunks.\n",
        "\n",
        "**Parameters:**\n",
        "- `chunk_size` (int): Maximum number of characters per chunk (default: 500)\n",
        "- `chunk_overlap` (int): Number of characters to overlap between chunks (default: 100)\n",
        "\n",
        "**Key method:**\n",
        "- `chunk(document_text)`: Returns a list of text chunks\n",
        "\n",
        "**Why it matters:** Chunk size affects both retrieval precision and context quality. Smaller chunks = more precise retrieval but less context. Larger chunks = more context but harder to find exact information.\n",
        "\n",
        "**Example usage:**\n",
        "```python\n",
        "# Default settings\n",
        "chunker = Chunker(chunk_size=500, chunk_overlap=100)\n",
        "\n",
        "# Experiment: Try larger chunks for more context\n",
        "chunker = Chunker(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# Experiment: Try smaller chunks for precision\n",
        "chunker = Chunker(chunk_size=300, chunk_overlap=50)"
      ],
      "metadata": {
        "id": "Lk5q1ZDEm9x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Chunker:\n",
        "    \"\"\"Handles document chunking - easily swappable\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=100):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap\n",
        "        )\n",
        "\n",
        "    def chunk(self, document_text):\n",
        "        \"\"\"Split document into chunks\"\"\"\n",
        "        return self.splitter.split_text(document_text)"
      ],
      "metadata": {
        "id": "ntOaeFcim_rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Embedder`\n",
        "\n",
        "Converts text into dense vector embeddings for semantic search.\n",
        "\n",
        "**Parameters:**\n",
        "- `model_name` (str): HuggingFace model name (default: `\"BAAI/bge-small-en-v1.5\"`)\n",
        "\n",
        "**Key method:**\n",
        "- `embed(texts)`: Returns tensor of embeddings\n",
        "\n",
        "**Model examples:**\n",
        "- `\"BAAI/bge-small-en-v1.5\"`: Fast, good quality (default).\n",
        "- `\"BAAI/bge-base-en-v1.5\"`: Slower, better quality.\n",
        "- Check the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), click retrieval and select a good model.\n",
        "\n",
        "**Why it matters:** Better embeddings = better semantic understanding = better retrieval. But larger models are slower.\n",
        "\n",
        "**Example usage:**\n",
        "\n",
        "```\n",
        "# Fast and good\n",
        "embedder = Embedder(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Better quality, slower\n",
        "embedder = Embedder(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OLvQK3VqnymP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedder:\n",
        "    \"\"\"Handles embedding - easily swappable\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"BAAI/bge-small-en-v1.5\"):\n",
        "        self.model_name = model_name\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def embed(self, texts):\n",
        "        \"\"\"Embed texts into vectors\"\"\"\n",
        "        return self.model.encode(texts, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "fdJR415MoRsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Retriever`\n",
        "\n",
        "Finds the most relevant chunks for a given query using embedding similarity.\n",
        "\n",
        "**Parameters:**\n",
        "- `embedder` (Embedder): The embedder instance to use for query encoding\n",
        "\n",
        "**Key method:**\n",
        "- `retrieve(query, chunks, chunk_embeddings, top_k)`: Returns top-k most relevant chunks\n",
        "\n",
        "**Why it matters:** This is the core of RAG - if retrieval fails, generation can't succeed. Uses cosine similarity between query and chunk embeddings.\n",
        "\n",
        "**Example usage:**\n",
        "\n",
        "\n",
        "```\n",
        "embedder = Embedder(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "retriever = Retriever(embedder)\n",
        "\n",
        "# Retrieve top 5 chunks\n",
        "relevant_chunks = retriever.retrieve(query, chunks, embeddings, top_k=5)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsZYkOAIoby0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Retriever:\n",
        "    \"\"\"Handles retrieval - easily swappable\"\"\"\n",
        "\n",
        "    def __init__(self, embedder):\n",
        "        self.embedder = embedder\n",
        "\n",
        "    def retrieve(self, query, chunks, chunk_embeddings, top_k=5):\n",
        "        \"\"\"Retrieve top-k most relevant chunks\"\"\"\n",
        "        query_embedding = self.embedder.embed(query)\n",
        "        similarities = util.pytorch_cos_sim(query_embedding, chunk_embeddings)\n",
        "\n",
        "        # Handle case where top_k is larger than number of chunks\n",
        "        actual_k = min(top_k, len(chunks))\n",
        "        top_k_indices = similarities[0].topk(actual_k).indices\n",
        "        return [chunks[i] for i in top_k_indices]"
      ],
      "metadata": {
        "id": "64p5YCicodkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Reranker`\n",
        "\n",
        "Re-scores retrieved chunks using a cross-encoder model for improved relevance ranking.\n",
        "\n",
        "**Parameters:**\n",
        "- `model_name` (str): Cross-encoder model name (default: `'cross-encoder/ms-marco-MiniLM-L-6-v2'`)\n",
        "\n",
        "**Key method:**\n",
        "- `rerank(query, chunks, top_k)`: Returns reranked chunks based on relevance scores\n",
        "\n",
        "**Popular models:**\n",
        "- `'cross-encoder/ms-marco-MiniLM-L-6-v2'`: Fast, good quality (default)\n",
        "- `'BAAI/bge-reranker-base'`: High quality, decent size\n",
        "- - Check the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), click re-ranking and select a good model.\n",
        "\n",
        "**Why it matters:** Embedding models retrieve candidates quickly but aren't perfect. Rerankers are slower but more accurate at scoring query-document pairs. Typical pattern: retrieve 15 candidates, rerank to get best 5.\n",
        "\n",
        "**Example usage:**\n",
        "\n",
        "\n",
        "```\n",
        "# Optional component - adds ~100-200ms latency but improves accuracy\n",
        "reranker = Reranker(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "# Use in pipeline\n",
        "pipeline = RAGPipeline(chunker, embedder, retriever, generator, reranker=reranker)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "0SgHw89crFV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Reranker:\n",
        "    \"\"\"Handles reranking of retrieved chunks - optional component\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'):\n",
        "        \"\"\"\n",
        "        Initialize reranker with a cross-encoder model.\n",
        "\n",
        "        Args:\n",
        "            model_name: HuggingFace model name for cross-encoder\n",
        "                       Popular options:\n",
        "                       - 'cross-encoder/ms-marco-MiniLM-L-6-v2' (fast, good)\n",
        "                       - 'BAAI/bge-reranker-base' (high quality, decent size)\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.model = CrossEncoder(model_name)\n",
        "\n",
        "    def rerank(self, query, chunks, top_k=None):\n",
        "        \"\"\"\n",
        "        Rerank chunks based on query-chunk relevance scores.\n",
        "\n",
        "        Args:\n",
        "            query: Search query\n",
        "            chunks: List of text chunks to rerank\n",
        "            top_k: Return only top_k after reranking (None = return all)\n",
        "\n",
        "        Returns:\n",
        "            List of reranked chunks\n",
        "        \"\"\"\n",
        "        # Create pairs of [query, chunk] for cross-encoder\n",
        "        pairs = [[query, chunk] for chunk in chunks]\n",
        "\n",
        "        # Get relevance scores\n",
        "        scores = self.model.predict(pairs)\n",
        "\n",
        "        # Sort chunks by score (descending)\n",
        "        ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
        "        reranked_chunks = [chunks[i] for i in ranked_indices]\n",
        "\n",
        "        # Return top_k if specified\n",
        "        if top_k:\n",
        "            return reranked_chunks[:top_k]\n",
        "        return reranked_chunks\n"
      ],
      "metadata": {
        "id": "NnvBupSKrFrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Generator`\n",
        "\n",
        "Generates natural language answers from retrieved context using an LLM.\n",
        "\n",
        "**Parameters:**\n",
        "- `model_name` (str): HuggingFace model name (default: `\"HuggingFaceTB/SmolLM-135M-Instruct\"`)\n",
        "- `batch_size` (int): Number of queries to process at once (default: 1)\n",
        "\n",
        "**Key method:**\n",
        "- `generate(query, context_chunks)`: Returns generated answer as string\n",
        "\n",
        "**Popular models:**\n",
        "- `\"HuggingFaceTB/SmolLM-135M-Instruct\"`: Tiny, fast, weak (default - good for testing)\n",
        "- Check the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/), look at overall performance (average score) and filter by size.\n",
        "- Check the [AlpacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/) for a quick overview of instruction-following LLMs, incl. closed models.\n",
        "\n",
        "**Why it matters:** The LLM is the final step - it must extract the right answer from the context. Small models often fail at this task. If retrieval accuracy >> generation accuracy, you need a better LLM.\n",
        "\n",
        "**Example usage:**\n",
        "\n",
        "```\n",
        "# Small model (fast but weak)\n",
        "generator = Generator(model_name=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n",
        "\n",
        "# Better model (slower but more accurate)\n",
        "generator = Generator(model_name=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "```"
      ],
      "metadata": {
        "id": "fqYxR49mrtE8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WMI-miYp_mE"
      },
      "outputs": [],
      "source": [
        "class Generator:\n",
        "    \"\"\"Handles answer generation - easily swappable\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"HuggingFaceTB/SmolLM-135M-Instruct\"):\n",
        "        device = 0 if torch.cuda.is_available() else -1\n",
        "        self.pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_name,\n",
        "            device=device\n",
        "        )\n",
        "        self.system_prompt = (\n",
        "            \"You are a helpful assistant that answers questions based on the given context. \"\n",
        "            \"Provide direct, concise answers.\"\n",
        "        )\n",
        "\n",
        "    def generate(self, query, context_chunks):\n",
        "        \"\"\"Generate answer from query and context\"\"\"\n",
        "        context = \"\\n\".join(context_chunks)\n",
        "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "\n",
        "        response = self.pipeline(messages, max_new_tokens=100)\n",
        "        return response[0][\"generated_text\"][-1][\"content\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `RAGPipeline`\n",
        "\n",
        "Combines all components into a complete RAG system.\n",
        "\n",
        "**Parameters:**\n",
        "- `chunker` (Chunker): Document chunking component\n",
        "- `embedder` (Embedder): Text embedding component\n",
        "- `retriever` (Retriever): Chunk retrieval component\n",
        "- `generator` (Generator): Answer generation component\n",
        "- `reranker` (Reranker, optional): Reranking component (default: None)\n",
        "\n",
        "**Key methods:**\n",
        "- `prepare_document(document_text)`: Chunks and embeds a document, returns (chunks, embeddings)\n",
        "- `query(query, chunks, embeddings, top_k)`: Runs full RAG pipeline, returns (answer, context_chunks)\n",
        "\n",
        "**Why it matters:** This is your complete RAG system. Swap any component to test different configurations. The pipeline automatically handles reranking if a reranker is provided.\n",
        "\n",
        "**Example usage:**\n",
        "\n",
        "```\n",
        "# Build a basic pipeline\n",
        "chunker = Chunker(chunk_size=500, chunk_overlap=100)\n",
        "embedder = Embedder(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "retriever = Retriever(embedder)\n",
        "generator = Generator(model_name=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n",
        "\n",
        "# Create pipeline WITHOUT reranking\n",
        "pipeline = RAGPipeline(chunker, embedder, retriever, generator)\n",
        "\n",
        "# Create pipeline WITH reranking\n",
        "reranker = Reranker(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "pipeline_with_reranking = RAGPipeline(chunker, embedder, retriever, generator, reranker=reranker)\n",
        "\n",
        "# Use the pipeline\n",
        "chunks, embeddings = pipeline.prepare_document(document_text)\n",
        "answer, context = pipeline.query(\"What is the secret fruit?\", chunks, embeddings, top_k=5)\n",
        "```"
      ],
      "metadata": {
        "id": "AokiQYfGroXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGPipeline:\n",
        "    \"\"\"Complete RAG pipeline - compose all components\"\"\"\n",
        "\n",
        "    def __init__(self, chunker, embedder, retriever, generator, reranker=None):\n",
        "        \"\"\"\n",
        "        Initialize RAG pipeline.\n",
        "\n",
        "        Args:\n",
        "            chunker: Chunker instance\n",
        "            embedder: Embedder instance\n",
        "            retriever: Retriever instance\n",
        "            generator: Generator instance\n",
        "            reranker: Optional Reranker instance (None = no reranking)\n",
        "        \"\"\"\n",
        "        self.chunker = chunker\n",
        "        self.embedder = embedder\n",
        "        self.retriever = retriever\n",
        "        self.generator = generator\n",
        "        self.reranker = reranker\n",
        "\n",
        "    def prepare_document(self, document_text):\n",
        "        \"\"\"Prepare document for retrieval\"\"\"\n",
        "        chunks = self.chunker.chunk(document_text)\n",
        "        embeddings = self.embedder.embed(chunks)\n",
        "        return chunks, embeddings\n",
        "\n",
        "    def query(self, query, chunks, chunk_embeddings, top_k=5, rerank_top_k=None):\n",
        "        \"\"\"\n",
        "        Run full RAG pipeline with optional reranking.\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            chunks: Document chunks\n",
        "            chunk_embeddings: Pre-computed embeddings\n",
        "            top_k: Number of chunks to retrieve initially\n",
        "            rerank_top_k: If reranker is used, return this many after reranking\n",
        "                         (None = return same as top_k)\n",
        "\n",
        "        Returns:\n",
        "            (answer, context_chunks) tuple\n",
        "        \"\"\"\n",
        "        # Step 1: Initial retrieval with embeddings\n",
        "        if self.reranker:\n",
        "            # Retrieve more candidates for reranking, but not more than available chunks\n",
        "            initial_k = min(top_k * 3, len(chunks))\n",
        "            context_chunks = self.retriever.retrieve(query, chunks, chunk_embeddings, initial_k)\n",
        "\n",
        "            # Step 2: Rerank the candidates\n",
        "            final_k = rerank_top_k if rerank_top_k else top_k\n",
        "            context_chunks = self.reranker.rerank(query, context_chunks, top_k=final_k)\n",
        "        else:\n",
        "            # No reranking - just retrieve\n",
        "            context_chunks = self.retriever.retrieve(query, chunks, chunk_embeddings, top_k)\n",
        "\n",
        "        # Step 3: Generate answer\n",
        "        answer = self.generator.generate(query, context_chunks)\n",
        "\n",
        "        return answer, context_chunks"
      ],
      "metadata": {
        "id": "9tWq3pKgrojM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `evaluate_rag(test_cases, rag_pipeline, top_k, verbose, use_llm)`\n",
        "\n",
        "Evaluates a RAG pipeline on the needle-in-haystack benchmark and reports performance metrics.\n",
        "\n",
        "**Parameters:**\n",
        "- `test_cases` (list): List of test case dictionaries from `load_test_cases()`\n",
        "- `rag_pipeline` (RAGPipeline): The RAG pipeline to evaluate\n",
        "- `top_k` (int): Number of chunks to retrieve per query (default: 5)\n",
        "- `verbose` (bool): If True, prints detailed progress for each needle (default: False)\n",
        "- `use_llm` (bool): If True, uses LLM to generate answers. If False, just checks if needle is in retrieved chunks (default: True)\n",
        "\n",
        "**Returns:**\n",
        "Dictionary with:\n",
        "- `accuracy`: Percentage of needles found\n",
        "- `correct`: Number of correct answers\n",
        "- `total`: Total number of test cases\n",
        "- `time`: Total evaluation time in seconds\n",
        "- `results`: Detailed results for each test case\n",
        "\n",
        "**Why it matters:** This function reveals where your RAG system struggles. The `use_llm` parameter is crucial for debugging:\n",
        "- `use_llm=False`: Tests **retrieval quality** - Can you find the right chunks?\n",
        "- `use_llm=True`: Tests **end-to-end performance** - Can the LLM extract the answer?\n",
        "\n",
        "If retrieval accuracy is high but end-to-end accuracy is low, your LLM is the bottleneck.\n",
        "\n",
        "**Example usage:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Quick evaluation (minimal output)\n",
        "results = evaluate_rag(test_cases, pipeline, top_k=5, verbose=False, use_llm=True)\n",
        "\n",
        "# Detailed evaluation (see each needle tested)\n",
        "results = evaluate_rag(test_cases, pipeline, top_k=5, verbose=True, use_llm=True)\n",
        "\n",
        "# Test retrieval quality only (faster, no LLM needed)\n",
        "results = evaluate_rag(test_cases, pipeline, top_k=5, verbose=False, use_llm=False)\n",
        "\n",
        "# Compare retrieval vs. generation\n",
        "retrieval_results = evaluate_rag(test_cases, pipeline, top_k=5, use_llm=False)\n",
        "full_rag_results = evaluate_rag(test_cases, pipeline, top_k=5, use_llm=True)\n",
        "print(f\"Retrieval accuracy: {retrieval_results['accuracy']:.1f}%\")\n",
        "print(f\"End-to-end accuracy: {full_rag_results['accuracy']:.1f}%\")\n",
        "```"
      ],
      "metadata": {
        "id": "IFkfe9oCttUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# 3. EVALUATION\n",
        "# ================================================================================\n",
        "\n",
        "def evaluate_rag(test_cases, rag_pipeline, top_k=5, verbose=False, use_llm=True):\n",
        "    \"\"\"\n",
        "    Evaluate RAG pipeline on needle-in-haystack test cases.\n",
        "\n",
        "    Args:\n",
        "        test_cases: List of test case dictionaries\n",
        "        rag_pipeline: RAGPipeline instance to evaluate\n",
        "        top_k: Number of chunks to retrieve\n",
        "        verbose: If True, print detailed progress\n",
        "        use_llm: If True, use LLM to generate answer. If False, just check if needle is in retrieved chunks.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation results\n",
        "    \"\"\"\n",
        "    if not test_cases:\n",
        "        print(\"ERROR: No test cases provided!\")\n",
        "        return {\"accuracy\": 0, \"correct\": 0, \"total\": 0, \"time\": 0, \"results\": []}\n",
        "\n",
        "    results = []\n",
        "    correct = 0\n",
        "    total = len(test_cases)\n",
        "\n",
        "    # Group by document for efficiency\n",
        "    by_document = {}\n",
        "    for case in test_cases:\n",
        "        doc_key = (case[\"document_name\"], case[\"document_length\"])\n",
        "        if doc_key not in by_document:\n",
        "            by_document[doc_key] = []\n",
        "        by_document[doc_key].append(case)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    if verbose:\n",
        "        mode = \"with LLM generation\" if use_llm else \"retrieval-only (no LLM)\"\n",
        "        print(f\"Evaluating {total} test cases ({mode})...\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "    for doc_key in sorted(by_document.keys()):\n",
        "        cases = by_document[doc_key]\n",
        "        doc_name, doc_length = doc_key\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nðŸ“„ {doc_name} - {doc_length} pages ({len(cases)} needles)\")\n",
        "\n",
        "        # Prepare document once\n",
        "        first_case = cases[0]\n",
        "        chunks, embeddings = rag_pipeline.prepare_document(first_case[\"full_document\"])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Chunked into {len(chunks)} chunks\")\n",
        "\n",
        "        # Test each needle\n",
        "        for i, case in enumerate(cases, 1):\n",
        "            expected_clean = case[\"expected_value\"].lower().strip()\n",
        "            expected_clean = expected_clean.replace('a \"', '').replace('an \"', '').replace('the \"', '').replace('\"', '').strip()\n",
        "\n",
        "            if use_llm:\n",
        "                # Use full RAG pipeline with LLM generation\n",
        "                answer, context_chunks = rag_pipeline.query(\n",
        "                    case[\"prompt\"],\n",
        "                    chunks,\n",
        "                    embeddings,\n",
        "                    top_k=top_k\n",
        "                )\n",
        "\n",
        "                # Check if expected value is in the generated answer\n",
        "                answer_lower = answer.lower()\n",
        "                found = expected_clean in answer_lower\n",
        "\n",
        "            else:\n",
        "                # Retrieval-only mode: just check if needle is in retrieved chunks\n",
        "                if rag_pipeline.reranker:\n",
        "                    # With reranker: retrieve more, then rerank\n",
        "                    initial_k = min(top_k * 3, len(chunks))\n",
        "                    context_chunks = rag_pipeline.retriever.retrieve(\n",
        "                        case[\"prompt\"],\n",
        "                        chunks,\n",
        "                        embeddings,\n",
        "                        initial_k\n",
        "                    )\n",
        "                    context_chunks = rag_pipeline.reranker.rerank(\n",
        "                        case[\"prompt\"],\n",
        "                        context_chunks,\n",
        "                        top_k=top_k\n",
        "                    )\n",
        "                else:\n",
        "                    # No reranker: just retrieve\n",
        "                    context_chunks = rag_pipeline.retriever.retrieve(\n",
        "                        case[\"prompt\"],\n",
        "                        chunks,\n",
        "                        embeddings,\n",
        "                        top_k=top_k\n",
        "                    )\n",
        "\n",
        "                # Check if expected value is in retrieved chunks\n",
        "                retrieved_text = \" \".join(context_chunks).lower()\n",
        "                found = expected_clean in retrieved_text\n",
        "\n",
        "                answer = \"[Retrieval-only mode - no answer generated]\"\n",
        "\n",
        "            if found:\n",
        "                correct += 1\n",
        "\n",
        "            results.append({\n",
        "                \"document_name\": doc_name,\n",
        "                \"document_length\": doc_length,\n",
        "                \"prompt\": case[\"prompt\"],\n",
        "                \"expected\": expected_clean,\n",
        "                \"answer\": answer,\n",
        "                \"found\": found\n",
        "            })\n",
        "\n",
        "            if verbose:\n",
        "                status = \"âœ“\" if found else \"âœ—\"\n",
        "                print(f\"   [{i}/{len(cases)}] {status} {case['key']}: expected '{expected_clean}'\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    mode_str = \"(with LLM)\" if use_llm else \"(retrieval-only)\"\n",
        "    print(f\"Mode: {mode_str}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
        "    print(f\"Time: {total_time:.2f}s ({total_time/total*1000:.0f}ms per query)\" if total > 0 else \"Time: 0.00s\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "        \"time\": total_time,\n",
        "        \"use_llm\": use_llm,\n",
        "        \"results\": results\n",
        "    }\n"
      ],
      "metadata": {
        "id": "M80upnf8qWPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example usage\n"
      ],
      "metadata": {
        "id": "6D0ku8zovG7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data (only need to do this once)\n",
        "print(\"Step 1: Downloading data...\")\n",
        "base_path = download_documents(\"GoldmanSachs\")  # Or \"AIG\", \"Tesla\", \"all\", etc.\n",
        "\n",
        "# Load test cases\n",
        "print(\"\\nStep 2: Loading test cases...\")\n",
        "test_cases = load_test_cases(base_path, \"GoldmanSachs\")\n",
        "print(f\"Loaded {len(test_cases)} test cases\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ysKxO1YvOwI",
        "outputId": "471b4b0f-44a2-4204-88dc-722d834e0035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Downloading data...\n",
            "Downloading documents: GoldmanSachs\n",
            "======================================================================\n",
            "\n",
            "ðŸ“„ Downloading GoldmanSachs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ“ GoldmanSachs downloaded\n",
            "\n",
            "Step 2: Loading test cases...\n",
            "Looking for documents in: haystack_data\n",
            "\n",
            "Processing GoldmanSachs...\n",
            "  âœ“ Loading 5-page document...\n",
            "    Added 5 test cases\n",
            "  âœ“ Loading 10-page document...\n",
            "    Added 10 test cases\n",
            "  âœ“ Loading 25-page document...\n",
            "    Added 25 test cases\n",
            "  âœ“ Loading 50-page document...\n",
            "    Added 25 test cases\n",
            "  âœ“ Loading 75-page document...\n",
            "    Added 25 test cases\n",
            "  âœ“ Loading 100-page document...\n",
            "    Added 25 test cases\n",
            "  âœ“ Loading 150-page document...\n",
            "    Added 25 test cases\n",
            "  âœ“ Loading 200-page document...\n",
            "    Added 25 test cases\n",
            "\n",
            "âœ“ Total test cases loaded: 165\n",
            "Loaded 165 test cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's initialize the RAG and do a quick example on a single document and check the generated output."
      ],
      "metadata": {
        "id": "wuCBLWKC2Q4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RAG components\n",
        "print(\"\\nStep 3: Initializing RAG pipeline WITHOUT reranking...\")\n",
        "# Build a basic pipeline\n",
        "chunker = Chunker(chunk_size=500, chunk_overlap=100)\n",
        "embedder = Embedder(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "retriever = Retriever(embedder)\n",
        "generator = Generator(model_name=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n",
        "\n",
        "# Create pipeline WITHOUT reranking\n",
        "pipeline = RAGPipeline(chunker, embedder, retriever, generator)\n",
        "\n",
        "# Let's try a single test example\n",
        "# Get a Goldman Sachs document from our test cases\n",
        "# Let's use the 10-page document as an example\n",
        "gs_10page_cases = [case for case in test_cases if case[\"document_length\"] == 10]\n",
        "sample_case = gs_10page_cases[0]  # Get first needle from 10-page doc\n",
        "\n",
        "# Prepare the document\n",
        "document_text = sample_case[\"full_document\"]\n",
        "chunks, embeddings = pipeline.prepare_document(document_text)\n",
        "\n",
        "print(f\"Document prepared: {len(chunks)} chunks created\")\n",
        "print(f\"Testing query: {sample_case['prompt']}\")\n",
        "print(f\"Expected answer: {sample_case['expected_value']}\")\n",
        "print()\n",
        "\n",
        "# Query WITHOUT reranking\n",
        "answer, context = pipeline.query(sample_case[\"prompt\"], chunks, embeddings, top_k=5)\n",
        "print(\"=\"*70)\n",
        "print(\"WITHOUT RERANKING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"\\nRetrieved context (first chunk):\")\n",
        "print(context[0][:200] + \"...\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxWWaINg11i-",
        "outputId": "90509f82-3bc3-4de7-e9a2-46442d7068e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Initializing RAG pipeline WITHOUT reranking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document prepared: 25 chunks created\n",
            "Testing query: What is the secret flower in the document?\n",
            "Expected answer: lavender\n",
            "\n",
            "======================================================================\n",
            "WITHOUT RERANKING\n",
            "======================================================================\n",
            "Answer: Here's the answer for you:\n",
            "\n",
            "The secret flower is \"lavender\".\n",
            "\n",
            "The secret tool is \"scissors\".\n",
            "The secret drink is \"milk\".\n",
            "\n",
            "The secret shape is a \"star\".\n",
            "The secret shape is a \"box\".\n",
            "\n",
            "The secret kitchen appliance is a \"toaster\".\n",
            "7\n",
            "The secret landmark is the \"Colosseum\".\n",
            "\n",
            "Question: What is the secret flower in the document?\n",
            "Answer:\n",
            "\n",
            "The secret flower\n",
            "\n",
            "Retrieved context (first chunk):\n",
            "Annual Report\n",
            "2023\n",
            "THE GOLDMAN SACHS GROUP , INC.\n",
            "The secret flower is \"lavender\".\n",
            "\n",
            "The secret tool is \"scissors\"....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM gets confused and returns all (?) the secrets it could find. The correct secret is the top one so we'll count this as a success, while aware that some work is clearly needed to get the generation to produce a more appropriate response. With some stricter prompting we might get it to return only one secret, but likely a more powerful LLM is needed.\n",
        "\n",
        "Let's evaluate our RAG pipelin on all the Goldman Sachs' pdfs of various lengths.\n",
        "\n",
        "First, we'll focus only on retrieval and skip the generation step."
      ],
      "metadata": {
        "id": "_js-UOrStz5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "print(\"\\nStep 4a: Running evaluation WITHOUT LLM generation...\")\n",
        "results_baseline = evaluate_rag(test_cases, pipeline, top_k=5, verbose=True, use_llm=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj5KfeTzqSq4",
        "outputId": "d33e8570-21ec-4972-a898-cd27060f0174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4a: Running evaluation WITHOUT LLM generation...\n",
            "Evaluating 165 test cases (retrieval-only (no LLM))...\n",
            "======================================================================\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 5 pages (5 needles)\n",
            "   Chunked into 10 chunks\n",
            "   [1/5] âœ“ flower: expected 'lavender'\n",
            "   [2/5] âœ“ tool: expected 'scissors'\n",
            "   [3/5] âœ“ shape: expected 'star'\n",
            "   [4/5] âœ“ clothing: expected 'dress'\n",
            "   [5/5] âœ“ office supply: expected 'envelope'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 10 pages (10 needles)\n",
            "   Chunked into 25 chunks\n",
            "   [1/10] âœ“ flower: expected 'lavender'\n",
            "   [2/10] âœ“ tool: expected 'scissors'\n",
            "   [3/10] âœ“ shape: expected 'star'\n",
            "   [4/10] âœ“ clothing: expected 'dress'\n",
            "   [5/10] âœ“ office supply: expected 'envelope'\n",
            "   [6/10] âœ“ fruit: expected 'grape'\n",
            "   [7/10] âœ“ drink: expected 'milk'\n",
            "   [8/10] âœ“ transportation: expected 'airplane'\n",
            "   [9/10] âœ“ landmark: expected 'colosseum'\n",
            "   [10/10] âœ“ kitchen appliance: expected 'toaster'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 25 pages (25 needles)\n",
            "   Chunked into 119 chunks\n",
            "   [1/25] âœ“ flower: expected 'lavender'\n",
            "   [2/25] âœ“ tool: expected 'scissors'\n",
            "   [3/25] âœ“ shape: expected 'star'\n",
            "   [4/25] âœ“ clothing: expected 'dress'\n",
            "   [5/25] âœ“ animal #2: expected 'koala'\n",
            "   [6/25] âœ“ office supply: expected 'envelope'\n",
            "   [7/25] âœ“ animal #5: expected 'rabbit'\n",
            "   [8/25] âœ— fruit: expected 'grape'\n",
            "   [9/25] âœ“ animal #4: expected 'horse'\n",
            "   [10/25] âœ“ object #3: expected 'plate'\n",
            "   [11/25] âœ“ drink: expected 'milk'\n",
            "   [12/25] âœ“ object #5: expected 'candle'\n",
            "   [13/25] âœ“ transportation: expected 'airplane'\n",
            "   [14/25] âœ“ landmark: expected 'colosseum'\n",
            "   [15/25] âœ“ object #4: expected 'mirror'\n",
            "   [16/25] âœ“ animal #3: expected 'owl'\n",
            "   [17/25] âœ“ animal #1: expected 'elephant'\n",
            "   [18/25] âœ“ kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ— object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ“ currency: expected 'rupee'\n",
            "   [22/25] âœ“ vegetable: expected 'mushroom'\n",
            "   [23/25] âœ— instrument: expected 'drum'\n",
            "   [24/25] âœ— object #2: expected 'watch'\n",
            "   [25/25] âœ— food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 50 pages (25 needles)\n",
            "   Chunked into 440 chunks\n",
            "   [1/25] âœ“ flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ“ shape: expected 'star'\n",
            "   [4/25] âœ“ clothing: expected 'dress'\n",
            "   [5/25] âœ“ animal #2: expected 'koala'\n",
            "   [6/25] âœ“ office supply: expected 'envelope'\n",
            "   [7/25] âœ“ animal #5: expected 'rabbit'\n",
            "   [8/25] âœ“ fruit: expected 'grape'\n",
            "   [9/25] âœ“ animal #4: expected 'horse'\n",
            "   [10/25] âœ“ object #3: expected 'plate'\n",
            "   [11/25] âœ“ drink: expected 'milk'\n",
            "   [12/25] âœ— object #5: expected 'candle'\n",
            "   [13/25] âœ“ transportation: expected 'airplane'\n",
            "   [14/25] âœ“ landmark: expected 'colosseum'\n",
            "   [15/25] âœ— object #4: expected 'mirror'\n",
            "   [16/25] âœ“ animal #3: expected 'owl'\n",
            "   [17/25] âœ“ animal #1: expected 'elephant'\n",
            "   [18/25] âœ“ kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ“ object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ“ currency: expected 'rupee'\n",
            "   [22/25] âœ“ vegetable: expected 'mushroom'\n",
            "   [23/25] âœ“ instrument: expected 'drum'\n",
            "   [24/25] âœ“ object #2: expected 'watch'\n",
            "   [25/25] âœ— food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 75 pages (25 needles)\n",
            "   Chunked into 752 chunks\n",
            "   [1/25] âœ“ flower: expected 'lavender'\n",
            "   [2/25] âœ“ tool: expected 'scissors'\n",
            "   [3/25] âœ“ shape: expected 'star'\n",
            "   [4/25] âœ“ clothing: expected 'dress'\n",
            "   [5/25] âœ“ animal #2: expected 'koala'\n",
            "   [6/25] âœ“ office supply: expected 'envelope'\n",
            "   [7/25] âœ“ animal #5: expected 'rabbit'\n",
            "   [8/25] âœ“ fruit: expected 'grape'\n",
            "   [9/25] âœ“ animal #4: expected 'horse'\n",
            "   [10/25] âœ— object #3: expected 'plate'\n",
            "   [11/25] âœ“ drink: expected 'milk'\n",
            "   [12/25] âœ“ object #5: expected 'candle'\n",
            "   [13/25] âœ“ transportation: expected 'airplane'\n",
            "   [14/25] âœ“ landmark: expected 'colosseum'\n",
            "   [15/25] âœ“ object #4: expected 'mirror'\n",
            "   [16/25] âœ“ animal #3: expected 'owl'\n",
            "   [17/25] âœ— animal #1: expected 'elephant'\n",
            "   [18/25] âœ“ kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ— object #1: expected 'door'\n",
            "   [20/25] âœ— sport: expected 'skiing'\n",
            "   [21/25] âœ“ currency: expected 'rupee'\n",
            "   [22/25] âœ“ vegetable: expected 'mushroom'\n",
            "   [23/25] âœ“ instrument: expected 'drum'\n",
            "   [24/25] âœ“ object #2: expected 'watch'\n",
            "   [25/25] âœ— food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 100 pages (25 needles)\n",
            "   Chunked into 1049 chunks\n",
            "   [1/25] âœ“ flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ“ shape: expected 'star'\n",
            "   [4/25] âœ— clothing: expected 'dress'\n",
            "   [5/25] âœ“ animal #2: expected 'koala'\n",
            "   [6/25] âœ“ office supply: expected 'envelope'\n",
            "   [7/25] âœ“ animal #5: expected 'rabbit'\n",
            "   [8/25] âœ“ fruit: expected 'grape'\n",
            "   [9/25] âœ“ animal #4: expected 'horse'\n",
            "   [10/25] âœ“ object #3: expected 'plate'\n",
            "   [11/25] âœ“ drink: expected 'milk'\n",
            "   [12/25] âœ“ object #5: expected 'candle'\n",
            "   [13/25] âœ“ transportation: expected 'airplane'\n",
            "   [14/25] âœ“ landmark: expected 'colosseum'\n",
            "   [15/25] âœ“ object #4: expected 'mirror'\n",
            "   [16/25] âœ“ animal #3: expected 'owl'\n",
            "   [17/25] âœ“ animal #1: expected 'elephant'\n",
            "   [18/25] âœ“ kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ“ object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ“ currency: expected 'rupee'\n",
            "   [22/25] âœ“ vegetable: expected 'mushroom'\n",
            "   [23/25] âœ“ instrument: expected 'drum'\n",
            "   [24/25] âœ“ object #2: expected 'watch'\n",
            "   [25/25] âœ“ food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 150 pages (25 needles)\n",
            "   Chunked into 1611 chunks\n",
            "   [1/25] âœ“ flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ“ shape: expected 'star'\n",
            "   [4/25] âœ“ clothing: expected 'dress'\n",
            "   [5/25] âœ— animal #2: expected 'koala'\n",
            "   [6/25] âœ“ office supply: expected 'envelope'\n",
            "   [7/25] âœ“ animal #5: expected 'rabbit'\n",
            "   [8/25] âœ“ fruit: expected 'grape'\n",
            "   [9/25] âœ“ animal #4: expected 'horse'\n",
            "   [10/25] âœ“ object #3: expected 'plate'\n",
            "   [11/25] âœ“ drink: expected 'milk'\n",
            "   [12/25] âœ“ object #5: expected 'candle'\n",
            "   [13/25] âœ“ transportation: expected 'airplane'\n",
            "   [14/25] âœ“ landmark: expected 'colosseum'\n",
            "   [15/25] âœ“ object #4: expected 'mirror'\n",
            "   [16/25] âœ“ animal #3: expected 'owl'\n",
            "   [17/25] âœ“ animal #1: expected 'elephant'\n",
            "   [18/25] âœ“ kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ“ object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ“ currency: expected 'rupee'\n",
            "   [22/25] âœ“ vegetable: expected 'mushroom'\n",
            "   [23/25] âœ“ instrument: expected 'drum'\n",
            "   [24/25] âœ“ object #2: expected 'watch'\n",
            "   [25/25] âœ— food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 200 pages (25 needles)\n",
            "   Chunked into 2129 chunks\n",
            "   [1/25] âœ“ flower: expected 'lavender'\n",
            "   [2/25] âœ“ tool: expected 'scissors'\n",
            "   [3/25] âœ“ shape: expected 'star'\n",
            "   [4/25] âœ“ clothing: expected 'dress'\n",
            "   [5/25] âœ“ animal #2: expected 'koala'\n",
            "   [6/25] âœ“ office supply: expected 'envelope'\n",
            "   [7/25] âœ— animal #5: expected 'rabbit'\n",
            "   [8/25] âœ“ fruit: expected 'grape'\n",
            "   [9/25] âœ“ animal #4: expected 'horse'\n",
            "   [10/25] âœ“ object #3: expected 'plate'\n",
            "   [11/25] âœ“ drink: expected 'milk'\n",
            "   [12/25] âœ“ object #5: expected 'candle'\n",
            "   [13/25] âœ— transportation: expected 'airplane'\n",
            "   [14/25] âœ“ landmark: expected 'colosseum'\n",
            "   [15/25] âœ— object #4: expected 'mirror'\n",
            "   [16/25] âœ— animal #3: expected 'owl'\n",
            "   [17/25] âœ“ animal #1: expected 'elephant'\n",
            "   [18/25] âœ“ kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ“ object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ“ currency: expected 'rupee'\n",
            "   [22/25] âœ“ vegetable: expected 'mushroom'\n",
            "   [23/25] âœ“ instrument: expected 'drum'\n",
            "   [24/25] âœ— object #2: expected 'watch'\n",
            "   [25/25] âœ“ food: expected 'fries'\n",
            "\n",
            "======================================================================\n",
            "RESULTS\n",
            "======================================================================\n",
            "Mode: (retrieval-only)\n",
            "Accuracy: 85.45% (141/165 correct)\n",
            "Time: 8.72s (53ms per query)\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "About 85% of needles found in the haystack - pretty strong baseline!\n",
        "\n",
        "Now, let's add the generation step:"
      ],
      "metadata": {
        "id": "oXt122neua6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 4b: Running evaluation WITH LLM generation...\")\n",
        "results_baseline = evaluate_rag(test_cases, pipeline, top_k=5, verbose=True, use_llm=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdUPsbnbtTDx",
        "outputId": "dc1a8996-c1f3-42ef-901d-24aef4a08f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4b: Running evaluation WITH LLM generation...\n",
            "Evaluating 165 test cases (with LLM generation)...\n",
            "======================================================================\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 5 pages (5 needles)\n",
            "   Chunked into 10 chunks\n",
            "   [1/5] âœ— flower: expected 'lavender'\n",
            "   [2/5] âœ— tool: expected 'scissors'\n",
            "   [3/5] âœ— shape: expected 'star'\n",
            "   [4/5] âœ— clothing: expected 'dress'\n",
            "   [5/5] âœ— office supply: expected 'envelope'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 10 pages (10 needles)\n",
            "   Chunked into 25 chunks\n",
            "   [1/10] âœ“ flower: expected 'lavender'\n",
            "   [2/10] âœ— tool: expected 'scissors'\n",
            "   [3/10] âœ— shape: expected 'star'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [4/10] âœ— clothing: expected 'dress'\n",
            "   [5/10] âœ— office supply: expected 'envelope'\n",
            "   [6/10] âœ— fruit: expected 'grape'\n",
            "   [7/10] âœ— drink: expected 'milk'\n",
            "   [8/10] âœ— transportation: expected 'airplane'\n",
            "   [9/10] âœ— landmark: expected 'colosseum'\n",
            "   [10/10] âœ— kitchen appliance: expected 'toaster'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 25 pages (25 needles)\n",
            "   Chunked into 119 chunks\n",
            "   [1/25] âœ— flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ— shape: expected 'star'\n",
            "   [4/25] âœ— clothing: expected 'dress'\n",
            "   [5/25] âœ— animal #2: expected 'koala'\n",
            "   [6/25] âœ— office supply: expected 'envelope'\n",
            "   [7/25] âœ— animal #5: expected 'rabbit'\n",
            "   [8/25] âœ— fruit: expected 'grape'\n",
            "   [9/25] âœ— animal #4: expected 'horse'\n",
            "   [10/25] âœ— object #3: expected 'plate'\n",
            "   [11/25] âœ— drink: expected 'milk'\n",
            "   [12/25] âœ— object #5: expected 'candle'\n",
            "   [13/25] âœ— transportation: expected 'airplane'\n",
            "   [14/25] âœ— landmark: expected 'colosseum'\n",
            "   [15/25] âœ— object #4: expected 'mirror'\n",
            "   [16/25] âœ“ animal #3: expected 'owl'\n",
            "   [17/25] âœ— animal #1: expected 'elephant'\n",
            "   [18/25] âœ— kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ— object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ— currency: expected 'rupee'\n",
            "   [22/25] âœ— vegetable: expected 'mushroom'\n",
            "   [23/25] âœ— instrument: expected 'drum'\n",
            "   [24/25] âœ— object #2: expected 'watch'\n",
            "   [25/25] âœ— food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 50 pages (25 needles)\n",
            "   Chunked into 440 chunks\n",
            "   [1/25] âœ— flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ— shape: expected 'star'\n",
            "   [4/25] âœ— clothing: expected 'dress'\n",
            "   [5/25] âœ— animal #2: expected 'koala'\n",
            "   [6/25] âœ— office supply: expected 'envelope'\n",
            "   [7/25] âœ— animal #5: expected 'rabbit'\n",
            "   [8/25] âœ— fruit: expected 'grape'\n",
            "   [9/25] âœ“ animal #4: expected 'horse'\n",
            "   [10/25] âœ— object #3: expected 'plate'\n",
            "   [11/25] âœ— drink: expected 'milk'\n",
            "   [12/25] âœ— object #5: expected 'candle'\n",
            "   [13/25] âœ— transportation: expected 'airplane'\n",
            "   [14/25] âœ— landmark: expected 'colosseum'\n",
            "   [15/25] âœ— object #4: expected 'mirror'\n",
            "   [16/25] âœ— animal #3: expected 'owl'\n",
            "   [17/25] âœ— animal #1: expected 'elephant'\n",
            "   [18/25] âœ— kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ— object #1: expected 'door'\n",
            "   [20/25] âœ— sport: expected 'skiing'\n",
            "   [21/25] âœ— currency: expected 'rupee'\n",
            "   [22/25] âœ— vegetable: expected 'mushroom'\n",
            "   [23/25] âœ— instrument: expected 'drum'\n",
            "   [24/25] âœ— object #2: expected 'watch'\n",
            "   [25/25] âœ— food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 75 pages (25 needles)\n",
            "   Chunked into 752 chunks\n",
            "   [1/25] âœ— flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ“ shape: expected 'star'\n",
            "   [4/25] âœ“ clothing: expected 'dress'\n",
            "   [5/25] âœ— animal #2: expected 'koala'\n",
            "   [6/25] âœ— office supply: expected 'envelope'\n",
            "   [7/25] âœ— animal #5: expected 'rabbit'\n",
            "   [8/25] âœ— fruit: expected 'grape'\n",
            "   [9/25] âœ— animal #4: expected 'horse'\n",
            "   [10/25] âœ— object #3: expected 'plate'\n",
            "   [11/25] âœ— drink: expected 'milk'\n",
            "   [12/25] âœ— object #5: expected 'candle'\n",
            "   [13/25] âœ— transportation: expected 'airplane'\n",
            "   [14/25] âœ— landmark: expected 'colosseum'\n",
            "   [15/25] âœ“ object #4: expected 'mirror'\n",
            "   [16/25] âœ“ animal #3: expected 'owl'\n",
            "   [17/25] âœ— animal #1: expected 'elephant'\n",
            "   [18/25] âœ“ kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ— object #1: expected 'door'\n",
            "   [20/25] âœ— sport: expected 'skiing'\n",
            "   [21/25] âœ— currency: expected 'rupee'\n",
            "   [22/25] âœ— vegetable: expected 'mushroom'\n",
            "   [23/25] âœ— instrument: expected 'drum'\n",
            "   [24/25] âœ— object #2: expected 'watch'\n",
            "   [25/25] âœ— food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 100 pages (25 needles)\n",
            "   Chunked into 1049 chunks\n",
            "   [1/25] âœ— flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ— shape: expected 'star'\n",
            "   [4/25] âœ— clothing: expected 'dress'\n",
            "   [5/25] âœ— animal #2: expected 'koala'\n",
            "   [6/25] âœ— office supply: expected 'envelope'\n",
            "   [7/25] âœ— animal #5: expected 'rabbit'\n",
            "   [8/25] âœ— fruit: expected 'grape'\n",
            "   [9/25] âœ— animal #4: expected 'horse'\n",
            "   [10/25] âœ“ object #3: expected 'plate'\n",
            "   [11/25] âœ“ drink: expected 'milk'\n",
            "   [12/25] âœ— object #5: expected 'candle'\n",
            "   [13/25] âœ— transportation: expected 'airplane'\n",
            "   [14/25] âœ“ landmark: expected 'colosseum'\n",
            "   [15/25] âœ— object #4: expected 'mirror'\n",
            "   [16/25] âœ— animal #3: expected 'owl'\n",
            "   [17/25] âœ— animal #1: expected 'elephant'\n",
            "   [18/25] âœ“ kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ“ object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ— currency: expected 'rupee'\n",
            "   [22/25] âœ— vegetable: expected 'mushroom'\n",
            "   [23/25] âœ“ instrument: expected 'drum'\n",
            "   [24/25] âœ— object #2: expected 'watch'\n",
            "   [25/25] âœ“ food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 150 pages (25 needles)\n",
            "   Chunked into 1611 chunks\n",
            "   [1/25] âœ— flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ— shape: expected 'star'\n",
            "   [4/25] âœ— clothing: expected 'dress'\n",
            "   [5/25] âœ— animal #2: expected 'koala'\n",
            "   [6/25] âœ— office supply: expected 'envelope'\n",
            "   [7/25] âœ“ animal #5: expected 'rabbit'\n",
            "   [8/25] âœ— fruit: expected 'grape'\n",
            "   [9/25] âœ— animal #4: expected 'horse'\n",
            "   [10/25] âœ— object #3: expected 'plate'\n",
            "   [11/25] âœ— drink: expected 'milk'\n",
            "   [12/25] âœ— object #5: expected 'candle'\n",
            "   [13/25] âœ— transportation: expected 'airplane'\n",
            "   [14/25] âœ— landmark: expected 'colosseum'\n",
            "   [15/25] âœ“ object #4: expected 'mirror'\n",
            "   [16/25] âœ— animal #3: expected 'owl'\n",
            "   [17/25] âœ— animal #1: expected 'elephant'\n",
            "   [18/25] âœ— kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ“ object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ“ currency: expected 'rupee'\n",
            "   [22/25] âœ— vegetable: expected 'mushroom'\n",
            "   [23/25] âœ— instrument: expected 'drum'\n",
            "   [24/25] âœ— object #2: expected 'watch'\n",
            "   [25/25] âœ— food: expected 'fries'\n",
            "\n",
            "ðŸ“„ GoldmanSachs - 200 pages (25 needles)\n",
            "   Chunked into 2129 chunks\n",
            "   [1/25] âœ— flower: expected 'lavender'\n",
            "   [2/25] âœ— tool: expected 'scissors'\n",
            "   [3/25] âœ— shape: expected 'star'\n",
            "   [4/25] âœ— clothing: expected 'dress'\n",
            "   [5/25] âœ— animal #2: expected 'koala'\n",
            "   [6/25] âœ— office supply: expected 'envelope'\n",
            "   [7/25] âœ— animal #5: expected 'rabbit'\n",
            "   [8/25] âœ— fruit: expected 'grape'\n",
            "   [9/25] âœ“ animal #4: expected 'horse'\n",
            "   [10/25] âœ— object #3: expected 'plate'\n",
            "   [11/25] âœ“ drink: expected 'milk'\n",
            "   [12/25] âœ“ object #5: expected 'candle'\n",
            "   [13/25] âœ— transportation: expected 'airplane'\n",
            "   [14/25] âœ“ landmark: expected 'colosseum'\n",
            "   [15/25] âœ— object #4: expected 'mirror'\n",
            "   [16/25] âœ— animal #3: expected 'owl'\n",
            "   [17/25] âœ— animal #1: expected 'elephant'\n",
            "   [18/25] âœ— kitchen appliance: expected 'toaster'\n",
            "   [19/25] âœ— object #1: expected 'door'\n",
            "   [20/25] âœ“ sport: expected 'skiing'\n",
            "   [21/25] âœ— currency: expected 'rupee'\n",
            "   [22/25] âœ— vegetable: expected 'mushroom'\n",
            "   [23/25] âœ“ instrument: expected 'drum'\n",
            "   [24/25] âœ— object #2: expected 'watch'\n",
            "   [25/25] âœ“ food: expected 'fries'\n",
            "\n",
            "======================================================================\n",
            "RESULTS\n",
            "======================================================================\n",
            "Mode: (with LLM)\n",
            "Accuracy: 17.58% (29/165 correct)\n",
            "Time: 412.78s (2502ms per query)\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below 20% (17.58%) - that's not very impressive. Compared to the retrieval results, this result suggests massive potential gains in generation part. For this illustration, we did rely on a tiny LLM. Time to bring in the big(ger) guns."
      ],
      "metadata": {
        "id": "Ic5XNbyNvOIx"
      }
    }
  ]
}